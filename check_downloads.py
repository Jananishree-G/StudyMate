#!/usr/bin/env python3
"""
Check IBM Granite Model Download Status
"""

import os
import sys
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_huggingface_cache():
    """Check HuggingFace cache directory"""
    print("ğŸ” Checking HuggingFace Cache Locations...")
    
    # Common cache locations
    cache_locations = [
        Path.home() / ".cache" / "huggingface",
        Path(os.environ.get("HF_HOME", "")) if os.environ.get("HF_HOME") else None,
        Path(os.environ.get("TRANSFORMERS_CACHE", "")) if os.environ.get("TRANSFORMERS_CACHE") else None,
        Path("models") / "transformers",  # Local project cache
    ]
    
    # Remove None values
    cache_locations = [loc for loc in cache_locations if loc is not None]
    
    found_caches = []
    
    for cache_dir in cache_locations:
        if cache_dir.exists():
            print(f"âœ… Found cache directory: {cache_dir}")
            found_caches.append(cache_dir)
            
            # List contents
            try:
                items = list(cache_dir.iterdir())
                print(f"   ğŸ“ Contains {len(items)} items")
                
                # Look for IBM Granite models
                for item in items:
                    if "granite" in item.name.lower() or "ibm" in item.name.lower():
                        print(f"   ğŸ¤– Found Granite-related: {item.name}")
                        
                        # Check size if it's a directory
                        if item.is_dir():
                            try:
                                size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                                size_gb = size / (1024**3)
                                print(f"      ğŸ“Š Size: {size_gb:.2f} GB")
                            except:
                                print(f"      ğŸ“Š Size: Unable to calculate")
                
            except PermissionError:
                print(f"   âŒ Permission denied to read {cache_dir}")
        else:
            print(f"âŒ Cache directory not found: {cache_dir}")
    
    return found_caches

def check_current_downloads():
    """Check if any downloads are currently in progress"""
    print("\nğŸ”„ Checking Current Download Status...")
    
    try:
        from transformers.utils import TRANSFORMERS_CACHE
        cache_path = Path(TRANSFORMERS_CACHE)
        
        print(f"ğŸ“ Default Transformers Cache: {cache_path}")
        
        if cache_path.exists():
            # Look for .incomplete files (partial downloads)
            incomplete_files = list(cache_path.rglob("*.incomplete"))
            if incomplete_files:
                print(f"ğŸ”„ Found {len(incomplete_files)} incomplete downloads:")
                for file in incomplete_files:
                    size = file.stat().st_size / (1024**2)  # MB
                    print(f"   ğŸ“¥ {file.name}: {size:.1f} MB")
            else:
                print("âœ… No incomplete downloads found")
            
            # Look for lock files (active downloads)
            lock_files = list(cache_path.rglob("*.lock"))
            if lock_files:
                print(f"ğŸ”’ Found {len(lock_files)} active download locks:")
                for file in lock_files:
                    print(f"   ğŸ”’ {file.name}")
            else:
                print("âœ… No active download locks found")
        
    except Exception as e:
        print(f"âŒ Error checking downloads: {e}")

def check_model_availability():
    """Check which IBM Granite models are available locally"""
    print("\nğŸ¤– Checking IBM Granite Model Availability...")
    
    granite_models = [
        "ibm-granite/granite-3b-code-instruct",
        "ibm-granite/granite-8b-code-instruct",
        "ibm-granite/granite-13b-instruct-v2"
    ]
    
    for model_id in granite_models:
        print(f"\nğŸ“‹ Checking: {model_id}")
        
        try:
            from transformers import AutoTokenizer
            
            # Try to load tokenizer (lightweight check)
            tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                trust_remote_code=True,
                local_files_only=True  # Only check local files
            )
            print(f"   âœ… Tokenizer available locally")
            
            # Try to check model files
            from transformers import AutoModelForCausalLM
            try:
                # Just check if model files exist locally
                model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    trust_remote_code=True,
                    local_files_only=True,
                    torch_dtype="auto"
                )
                print(f"   âœ… Model files available locally")
                
                # Get model size estimate
                param_count = sum(p.numel() for p in model.parameters())
                size_gb = param_count * 4 / (1024**3)  # Rough estimate (4 bytes per param)
                print(f"   ğŸ“Š Estimated size: {size_gb:.1f} GB")
                
                del model  # Free memory
                
            except Exception as e:
                print(f"   âŒ Model files not available locally: {e}")
                
        except Exception as e:
            print(f"   âŒ Not available locally: {e}")

def main():
    """Main function"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                              â•‘
    â•‘   ğŸ” IBM Granite Model Download Status Checker                              â•‘
    â•‘                                                                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Check cache directories
    found_caches = check_huggingface_cache()
    
    # Check current downloads
    check_current_downloads()
    
    # Check model availability
    check_model_availability()
    
    # Summary
    print("\n" + "="*60)
    print("ğŸ“Š SUMMARY")
    print("="*60)
    
    if found_caches:
        print(f"âœ… Found {len(found_caches)} cache directories")
        for cache in found_caches:
            print(f"   ğŸ“ {cache}")
    else:
        print("âŒ No HuggingFace cache directories found")
    
    print("\nğŸ’¡ TIPS:")
    print("â€¢ Models download to: ~/.cache/huggingface/hub/")
    print("â€¢ Large models (7GB+) take time to download")
    print("â€¢ Check your internet connection if downloads are slow")
    print("â€¢ Downloads resume automatically if interrupted")

if __name__ == "__main__":
    main()
